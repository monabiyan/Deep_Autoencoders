{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sorting_deep_learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/monabiyan/Deep_Autoencoders/blob/master/sorting_deep_learning.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "6wjAn4EQYkHe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3924
        },
        "outputId": "700967a5-0b34-4f90-952a-c2e36d007307"
      },
      "cell_type": "code",
      "source": [
        "def sorting_deep_learning():\n",
        "    import random\n",
        "    from random import randint\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Dense, Dropout\n",
        "    from keras.models import load_model\n",
        "    from keras import backend as K\n",
        "    from keras.utils.generic_utils import get_custom_objects\n",
        "    from keras.layers import Activation\n",
        "    from keras import regularizers\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    all_x=[]\n",
        "    all_y=[]\n",
        "    nn=100000\n",
        "    max_no=1000\n",
        "    for i in range(nn):\n",
        "        if i % 1000 == 0:\n",
        "            print(i)\n",
        "        my_list=np.random.choice(randint(1,nn),max_no,replace=True)\n",
        "        all_x.append(my_list)\n",
        "        all_y.append(sorted(my_list))\n",
        "\n",
        "\n",
        "\n",
        "    all_x=np.array(all_x)/nn\n",
        "    all_y=np.array(all_y)/nn\n",
        "\n",
        "    all_x_train=all_x[0:int(nn/1.2)]\n",
        "    all_y_train = all_y[0:int(nn/1.2)]\n",
        "    all_x_test = all_x[int(nn / 1.2) +1:nn]\n",
        "    all_y_test = all_y[int(nn / 1.2) +1:nn]\n",
        "    \n",
        "    print('omid')\n",
        "    print(all_x.shape)\n",
        "    print(all_y.shape)\n",
        "    print('baba')\n",
        "\n",
        "\n",
        "\n",
        "    #######  custom_activation  ################################\n",
        "    def custom_activation(x):\n",
        "        return (K.round(x))\n",
        "    get_custom_objects().update({'custom_activation': Activation(custom_activation)})\n",
        "    #######  custom_activation finish ################################\n",
        "    def customLoss(yTrue,yPred):\n",
        "      import math\n",
        "      return K.mean((K.abs(np.divide(yTrue-yPred,yTrue+0.00001))))\n",
        "      #return K.sum(K.log(K.abs(yTrue-yPred)+math.e)-1)\n",
        "#       return K.sum(K.cast(K.not_equal(K.round(yTrue),K.round(yPred)),'float32'))\n",
        "      #return K.sum(K.pow(K.abs(yTrue-yPred),0.1))\n",
        "\n",
        "#     drop_out_value=0.2\n",
        "    model = Sequential()\n",
        "    model.add(Dense(20, activation='relu', input_dim=max_no))\n",
        "#     model.add(Dropout(0.05))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "#     model.add(Dropout(0.2))\n",
        "#     model.add(Dense(10, activation='relu'))\n",
        "#     model.add(Dense(10, activation='relu'))\n",
        "#     model.add(Dense(10, activation='relu'))\n",
        "#     model.add(Dense(int(max_no/2), activation='relu'))\n",
        "#     model.add(Dropout(0.2))\n",
        "    model.add(Dense(max_no, activation='linear'))\n",
        "#       ,kernel_regularizer=regularizers.l2(0.01)\n",
        "    # model.compile(loss='mean_squared_error', optimizer='sgd')\n",
        "#     model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mae'])\n",
        "#     model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "    model.compile(optimizer='adam', loss=customLoss, metrics=['mae'])\n",
        "    model.summary()\n",
        "    history=model.fit(all_x_train,all_y_train, epochs=100, batch_size=512,validation_split=1/12)\n",
        "    prediction = model.predict(all_x_test)\n",
        "    \n",
        "    print('accuracy')\n",
        "    print(np.sum(np.abs(np.subtract(all_y_test,prediction)))/(nn*max_no/2))\n",
        "    print('vs')\n",
        "    print(np.sum(np.abs(np.subtract(all_y_test,all_x_test)))/(nn*max_no/2))\n",
        "    \n",
        "#     print(all_x_test[0])\n",
        "#     print(model.predict(all_x_test)[0].astype(int))\n",
        "#     model.save('sorting_model.h5')\n",
        "#     model1a = load_model('sorting_model.h5')\n",
        "\n",
        "    import time\n",
        "\n",
        "    start_time = time.time()\n",
        "    prediction2 = model.predict(all_x_test)\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "    start_time = time.time()\n",
        "    algo_sort=np.sort(all_x_test)\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "    pr=prediction2[0]\n",
        "    \n",
        "    print(pr*nn)\n",
        "    print(algo_sort[0]*nn)\n",
        "    print(algo_sort[0]*nn-pr*nn)\n",
        "    \n",
        "    plt.subplot(6,1,1)\n",
        "    plt.plot(all_x_test[0]*nn)\n",
        "    plt.subplot(6, 1, 2)\n",
        "    plt.plot(pr*nn)\n",
        "    plt.subplot(6, 1, 3)\n",
        "    plt.plot(algo_sort[0]*nn)\n",
        "    plt.show()\n",
        "    plt.subplot(6, 1, 4)\n",
        "    plt.plot(np.subtract(pr,algo_sort[0])*nn)\n",
        "    plt.show()\n",
        "#     plt.subplot(6,1,5)\n",
        "#     plt.plot(history.history['acc'])\n",
        "#     # plt.plot(history.history['val_acc'])\n",
        "#     plt.title('model accuracy')\n",
        "#     plt.ylabel('accuracy')\n",
        "#     plt.xlabel('epoch')\n",
        "#     plt.legend(['train', 'test'], loc='upper left')\n",
        "    # summarize history for loss\n",
        "    plt.subplot(6, 1, 5)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "    \n",
        "sorting_deep_learning()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n",
            "68000\n",
            "69000\n",
            "70000\n",
            "71000\n",
            "72000\n",
            "73000\n",
            "74000\n",
            "75000\n",
            "76000\n",
            "77000\n",
            "78000\n",
            "79000\n",
            "80000\n",
            "81000\n",
            "82000\n",
            "83000\n",
            "84000\n",
            "85000\n",
            "86000\n",
            "87000\n",
            "88000\n",
            "89000\n",
            "90000\n",
            "91000\n",
            "92000\n",
            "93000\n",
            "94000\n",
            "95000\n",
            "96000\n",
            "97000\n",
            "98000\n",
            "99000\n",
            "omid\n",
            "(100000, 1000)\n",
            "(100000, 1000)\n",
            "baba\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_72 (Dense)             (None, 20)                20020     \n",
            "_________________________________________________________________\n",
            "dense_73 (Dense)             (None, 50)                1050      \n",
            "_________________________________________________________________\n",
            "dense_74 (Dense)             (None, 1000)              51000     \n",
            "=================================================================\n",
            "Total params: 72,070\n",
            "Trainable params: 72,070\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 76388 samples, validate on 6945 samples\n",
            "Epoch 1/100\n",
            "76388/76388 [==============================] - 2s 30us/step - loss: -3083365875.3902 - mean_absolute_error: 263.7362 - val_loss: -14764437310.4058 - val_mean_absolute_error: 1278.1997\n",
            "Epoch 2/100\n",
            "38912/76388 [==============>...............] - ETA: 0s - loss: -34846357423.1579 - mean_absolute_error: 3021.8213"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "76388/76388 [==============================] - 1s 19us/step - loss: -70359078199.1626 - mean_absolute_error: 6160.9673 - val_loss: -163911132425.2521 - val_mean_absolute_error: 14339.2571\n",
            "Epoch 3/100\n",
            "76388/76388 [==============================] - 1s 19us/step - loss: -354074451956.4178 - mean_absolute_error: 31053.0584 - val_loss: -608159187960.7753 - val_mean_absolute_error: 53309.7100\n",
            "Epoch 4/100\n",
            "76388/76388 [==============================] - 1s 19us/step - loss: -1004376206636.9746 - mean_absolute_error: 88159.3924 - val_loss: -1484056847672.4343 - val_mean_absolute_error: 130239.9010\n",
            "Epoch 5/100\n",
            "76388/76388 [==============================] - 1s 18us/step - loss: -2146221923430.1479 - mean_absolute_error: 188862.5488 - val_loss: -2905315612435.2046 - val_mean_absolute_error: 255092.9097\n",
            "Epoch 6/100\n",
            "76388/76388 [==============================] - 1s 19us/step - loss: -3885773666595.8057 - mean_absolute_error: 341734.0756 - val_loss: -4965916941664.8340 - val_mean_absolute_error: 436181.4522\n",
            "Epoch 7/100\n",
            "74752/76388 [============================>.] - ETA: 0s - loss: -6285512563501.5889 - mean_absolute_error: 552705.9022"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "76388/76388 [==============================] - 1s 19us/step - loss: -6311664868072.3662 - mean_absolute_error: 555484.5372 - val_loss: -7749082466306.0654 - val_mean_absolute_error: 680787.0656\n",
            "Epoch 8/100\n",
            "76388/76388 [==============================] - 1s 19us/step - loss: -9496264082525.0332 - mean_absolute_error: 836395.4825 - val_loss: -11320344455459.0547 - val_mean_absolute_error: 994784.8805\n",
            "Epoch 9/100\n",
            "76388/76388 [==============================] - 1s 19us/step - loss: -13506611835595.7324 - mean_absolute_error: 1188587.6655 - val_loss: -15740532179734.3008 - val_mean_absolute_error: 1383367.2591\n",
            "Epoch 10/100\n",
            "76388/76388 [==============================] - 1s 19us/step - loss: -18399136676263.0703 - mean_absolute_error: 1618718.7315 - val_loss: -21062698108356.8008 - val_mean_absolute_error: 1851213.9436\n",
            "Epoch 11/100\n",
            "76388/76388 [==============================] - 1s 19us/step - loss: -24214638644208.8242 - mean_absolute_error: 2131548.5054 - val_loss: -27333509096352.3086 - val_mean_absolute_error: 2402363.3858\n",
            "Epoch 12/100\n",
            "72192/76388 [===========================>..] - ETA: 0s - loss: -30853217631907.4023 - mean_absolute_error: 2709819.2819"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "76388/76388 [==============================] - 1s 19us/step - loss: -31004214882093.1641 - mean_absolute_error: 2730091.4943 - val_loss: -34574040210336.6055 - val_mean_absolute_error: 3039192.2041\n",
            "Epoch 13/100\n",
            "76388/76388 [==============================] - 1s 19us/step - loss: -38792259129306.8984 - mean_absolute_error: 3415268.3006 - val_loss: -42834844450665.0156 - val_mean_absolute_error: 3765604.1142\n",
            "Epoch 14/100\n",
            "76388/76388 [==============================] - 1s 18us/step - loss: -47628756591576.4766 - mean_absolute_error: 4192083.5584 - val_loss: -52145275434503.4531 - val_mean_absolute_error: 4584291.7369\n",
            "Epoch 15/100\n",
            "76388/76388 [==============================] - 1s 19us/step - loss: -57533405080691.6641 - mean_absolute_error: 5064705.3018 - val_loss: -62532535598856.2969 - val_mean_absolute_error: 5497581.6990\n",
            "Epoch 16/100\n",
            "76388/76388 [==============================] - 1s 19us/step - loss: -68520956951338.8516 - mean_absolute_error: 6031285.4288 - val_loss: -74014509073087.0156 - val_mean_absolute_error: 6507478.5689\n",
            "Epoch 17/100\n",
            "76388/76388 [==============================] - 1s 18us/step - loss: -80611837753215.4062 - mean_absolute_error: 7099138.3064 - val_loss: -86619704526083.3594 - val_mean_absolute_error: 7616139.8078\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/100\n",
            "76388/76388 [==============================] - 1s 18us/step - loss: -93869117856522.6875 - mean_absolute_error: 8266286.2732 - val_loss: -100369011652983.4062 - val_mean_absolute_error: 8825354.7617\n",
            "Epoch 19/100\n",
            "76388/76388 [==============================] - 1s 19us/step - loss: -108281947702053.6094 - mean_absolute_error: 9536506.6100 - val_loss: -115278815482735.3594 - val_mean_absolute_error: 10136948.1788\n",
            "Epoch 20/100\n",
            "76388/76388 [==============================] - 1s 18us/step - loss: -123913990862080.2656 - mean_absolute_error: 10909428.8211 - val_loss: -131396629825431.9062 - val_mean_absolute_error: 11554170.6922\n",
            "Epoch 21/100\n",
            "76388/76388 [==============================] - 1s 19us/step - loss: -140703684923179.0312 - mean_absolute_error: 12388650.7909 - val_loss: -148694985808531.6562 - val_mean_absolute_error: 13075921.1971\n",
            "Epoch 22/100\n",
            "76388/76388 [==============================] - 1s 19us/step - loss: -158746867408157.7500 - mean_absolute_error: 13977347.7700 - val_loss: -167261337714853.1250 - val_mean_absolute_error: 14707998.2521\n",
            "Epoch 23/100\n",
            "52736/76388 [===================>..........] - ETA: 0s - loss: -174660858827905.2500 - mean_absolute_error: 15418193.8447"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "76388/76388 [==============================] - 1s 18us/step - loss: -178018893959739.1875 - mean_absolute_error: 15672880.1914 - val_loss: -187040093996399.8750 - val_mean_absolute_error: 16447713.0072\n",
            "Epoch 24/100\n",
            "76388/76388 [==============================] - 1s 19us/step - loss: -198529900027443.7188 - mean_absolute_error: 17483243.1400 - val_loss: -208087954635231.1875 - val_mean_absolute_error: 18298727.6190\n",
            "Epoch 25/100\n",
            "76388/76388 [==============================] - 1s 19us/step - loss: -220358109687477.0625 - mean_absolute_error: 19408565.4713 - val_loss: -230419610119204.1250 - val_mean_absolute_error: 20263095.7140\n",
            "Epoch 26/100\n",
            "76388/76388 [==============================] - 1s 19us/step - loss: -243475017982039.9375 - mean_absolute_error: 21442522.6599 - val_loss: -254053032173109.1562 - val_mean_absolute_error: 22341610.9224\n",
            "Epoch 27/100\n",
            "76388/76388 [==============================] - 1s 19us/step - loss: -267935668806635.3438 - mean_absolute_error: 23593433.0466 - val_loss: -279007107192845.2812 - val_mean_absolute_error: 24536709.0180\n",
            "Epoch 28/100\n",
            "59904/76388 [======================>.......] - ETA: 0s - loss: -290705942771519.4375 - mean_absolute_error: 25629087.5214"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "76388/76388 [==============================] - 1s 19us/step - loss: -293598514710329.6875 - mean_absolute_error: 25860303.3426 - val_loss: -305274692767281.0000 - val_mean_absolute_error: 26847815.4753\n",
            "Epoch 29/100\n",
            " 3584/76388 [>.............................] - ETA: 1s - loss: -311597427851264.0000 - mean_absolute_error: 27296361.4286"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-835286ea568c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0msorting_deep_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-835286ea568c>\u001b[0m in \u001b[0;36msorting_deep_learning\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustomLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_x_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_x_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1222\u001b[0m                             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m                             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m                         raise TypeError('TypeError while preparing batch. '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_slice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "uoE6b4lYIv6S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}